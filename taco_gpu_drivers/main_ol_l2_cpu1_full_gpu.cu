#include <cuda_runtime.h>
#include <stdlib.h>
#include <stdio.h>
#include <stdint.h>
#include <time.h>
#include "taco.h"

extern taco_tensor_t* init_taco_tensor(int32_t order, int32_t csize, int32_t* dimensions);
extern taco_tensor_t* init_taco_tensor_gpu(taco_tensor_t* ht);
extern void fill_array(int* arr, int len);
extern double calc_spent_time(struct timespec end, struct timespec start);
extern double average(double* values, int len);
extern void gpuAssert(cudaError_t code, const char *file, int line, bool abort);
#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }

#ifndef TACO_TENSOR_T_DEFINED
#define TACO_TENSOR_T_DEFINED
typedef enum { taco_mode_dense, taco_mode_sparse } taco_mode_t;
typedef struct {
    int32_t      order;
    int32_t*     dimensions;
    int32_t      csize;
    int32_t*     mode_ordering;
    taco_mode_t* mode_types;
    uint8_t***   indices;
    int*         vals; // Changed to int for prediction and truth tensors
    uint8_t*     fill_value;
    int32_t      vals_size;
} taco_tensor_t;
#endif

// Generated by the Tensor Algebra Compiler (tensor-compiler.org)

__global__
void computeDeviceKernel0(taco_tensor_t * __restrict__ a, taco_tensor_t * __restrict__ b, taco_tensor_t * __restrict__ c){
  float* __restrict__ a_vals = (float*)(a->vals);
  float* __restrict__ b_vals = (float*)(b->vals);
  int c1_dimension = (int)(c->dimensions[0]);
  float* __restrict__ c_vals = (float*)(c->vals);

  int32_t i67 = blockIdx.x;
  int32_t i68 = (threadIdx.x % (256));
  if (threadIdx.x >= 256) {
    return;
  }

  int32_t i = i67 * 256 + i68;
  if (i >= c1_dimension)
    return;

  a_vals[i] = (b_vals[i] - c_vals[i]) * (b_vals[i] - c_vals[i]);
}

int compute(taco_tensor_t *a, taco_tensor_t *b, taco_tensor_t *c) {
  int c1_dimension = (int)(c->dimensions[0]);

  computeDeviceKernel0<<<((c1_dimension + 255) / 256), 256>>>(a, b, c);
  cudaDeviceSynchronize();
  return 0;
}



int main(int argc, char* argv[]) {
    if (argc < 2) {
        printf("Please specify the number of runs.\n");
        exit(1);
    }

    int n_runs = atoi(argv[1]);
    srand(time(0));

    struct timespec start, end_orig, end_taco;
    double* orig_run_times = (double*)malloc(n_runs * sizeof(double));
    double* taco_run_times = (double*)malloc(n_runs * sizeof(double));

    int N = 1000000;
    int* pred = (int*)malloc(N * sizeof(int));
    int* truth = (int*)malloc(N * sizeof(int));
    int* error = (int*)malloc(N * sizeof(int));

    int dims[1] = {N};
    taco_tensor_t* pred_tt = init_taco_tensor(1, sizeof(int), dims);
    pred_tt->vals = pred;
    pred_tt = init_taco_tensor_gpu(pred_tt);

    taco_tensor_t* truth_tt = init_taco_tensor(1, sizeof(int), dims);
    truth_tt->vals = truth;
    truth_tt = init_taco_tensor_gpu(truth_tt);

    taco_tensor_t* error_tt = init_taco_tensor(1, sizeof(int), dims);
    error_tt->vals = error;
    error_tt = init_taco_tensor_gpu(error_tt);

    for (int i = 0; i < n_runs; i++) {
        fill_array(pred, N);
        fill_array(truth, N);
        fill_array(error, N); // Optional: To clear previous error values

        clock_gettime(CLOCK_MONOTONIC, &start);
        // Here you could run the original CPU version if available.
        ol_l2_cpu1(N, pred, truth, error); // Uncomment if you have the original function for comparison
        clock_gettime(CLOCK_MONOTONIC, &end_orig);

        compute(pred_tt, truth_tt, error_tt);
        clock_gettime(CLOCK_MONOTONIC, &end_taco);

        orig_run_times[i] = calc_spent_time(end_orig, start);
        taco_run_times[i] = calc_spent_time(end_taco, end_orig);
    }

    double orig_time = average(orig_run_times, n_runs);
    double taco_time = average(taco_run_times, n_runs);
    printf("%.5lf %.5lf\n", orig_time, taco_time);

    free(orig_run_times);
    free(taco_run_times);
    free(pred);
    free(truth);
    free(error);

    return 0;
}

