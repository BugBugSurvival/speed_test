#include <cuda_runtime.h>
#include <stdlib.h>
#include <stdio.h>
#include <stdint.h>
 #include <time.h>
#include "taco.h"
extern taco_tensor_t* init_taco_tensor(int32_t order, int32_t csize, int32_t* dimensions);
extern taco_tensor_t* init_taco_tensor_gpu(taco_tensor_t* ht);
extern void fill_array(float* arr, int len);
extern double calc_spent_time(struct timespec end, struct timespec start);
extern double average(double* values, int len);
extern void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true);
#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }

#ifndef TACO_TENSOR_T_DEFINED
#define TACO_TENSOR_T_DEFINED
typedef enum { taco_mode_dense, taco_mode_sparse } taco_mode_t;
typedef struct {
  int32_t      order;         // 
  int32_t*     dimensions;    // 
  int32_t      csize;         // 
  int32_t*     mode_ordering; // 
  taco_mode_t* mode_types;    // 
  uint8_t***   indices;       // 
  float* vals          // 
  uint8_t*     fill_value;    // 
  int32_t      vals_size;     // 
} taco_tensor_t;
#endif

// Generated by the Tensor Algebra Compiler (tensor-compiler.org)

__global__
void computeDeviceKernel0(taco_tensor_t * __restrict__ a, taco_tensor_t * __restrict__ b, float &c_val, taco_tensor_t * __restrict__ d, taco_tensor_t * __restrict__ e){
  int a2_dimension = (int)(a->dimensions[1]);
  float* __restrict__ a_vals = (float*)(a->vals);
  int b1_dimension = (int)(b->dimensions[0]);
  int b2_dimension = (int)(b->dimensions[1]);
  float* __restrict__ b_vals = (float*)(b->vals);
  int d2_dimension = (int)(d->dimensions[1]);
  float* __restrict__ d_vals = (float*)(d->vals);
  int e2_dimension = (int)(e->dimensions[1]);
  float* __restrict__ e_vals = (float*)(e->vals);

  int32_t i105 = blockIdx.x;
  int32_t i106 = (threadIdx.x % (256));
  if (threadIdx.x >= 256) {
    return;
  }

  int32_t i = i105 * 256 + i106;
  if (i >= b1_dimension)
    return;

  for (int32_t j = 0; j < b2_dimension; j++) {
    int32_t ja = i * a2_dimension + j;
    int32_t jb = i * b2_dimension + j;
    float tk_val = 0.0;
    for (int32_t k = 0; k < e2_dimension; k++) {
      int32_t id = k * d2_dimension + i;
      int32_t ke = j * e2_dimension + k;
      tk_val = tk_val + (c_val * d_vals[id]) * e_vals[ke];
    }
    a_vals[ja] = b_vals[jb] + tk_val;
  }
}

int compute(taco_tensor_t *a, taco_tensor_t *c, taco_tensor_t *d, taco_tensor_t *e, taco_tensor_t *b) {
  float* __restrict__ c_vals = (float*)(c->vals);
  int b1_dimension = (int)(b->dimensions[0]);

  float* c_val_ptr;
  gpuErrchk(cudaMallocManaged((void**)&c_val_ptr, sizeof(float)));
  float& c_val = *c_val_ptr;
  c_val = c_vals[0];

  computeDeviceKernel0<<<((b1_dimension + 255) / 256), 256>>>(a, b, c_val, d, e);
  cudaDeviceSynchronize();
  return 0;
}

int main(int argc, char* argv[]){
  int n_runs = atoi(argv[1]);
  if(argc < 2){
    printf("Please specify number of executions!\n");
    exit(1);
  }
  srand(time(0));
  struct timespec start, end_orig, end_taco;
  double* orig_run_times = (double*)malloc(n_runs * sizeof(double));
  double* taco_run_times = (double*)malloc(n_runs * sizeof(double));
  int M = 1000;
  int N = 1000;
  int K = 1000;
  float ALPHA = 1 + ((float)rand()) / RAND_MAX;
  float* A = (float*)malloc(1000 * 1000 * sizeof(float));
  int lda = 1000;
  float* B = (float*)malloc(1000 * 1000 * sizeof(float));
  int ldb = 1000;
  float* C = (float*)malloc(1000 * 1000 * sizeof(float));
  int ldc = 1000;

  int ALPHA_dims[1] = {1};
  taco_tensor_t* ALPHA_tt = init_taco_tensor(1, sizeof(float), ALPHA_dims);
  ALPHA_tt->vals = &ALPHA;
  ALPHA_tt = init_taco_tensor_gpu(ALPHA_tt);
  int A_dims[2] = {1000,1000};
  taco_tensor_t* A_tt = init_taco_tensor(2, sizeof(float), A_dims);
  A_tt->vals = A;
  A_tt = init_taco_tensor_gpu(A_tt);
  int B_dims[2] = {1000,1000};
  taco_tensor_t* B_tt = init_taco_tensor(2, sizeof(float), B_dims);
  B_tt->vals = B;
  B_tt = init_taco_tensor_gpu(B_tt);
  int C_dims[2] = {1000,1000};
  taco_tensor_t* C_tt = init_taco_tensor(2, sizeof(float), C_dims);
  C_tt->vals = C;
  C_tt = init_taco_tensor_gpu(C_tt);

  for(int i = 0; i < n_runs; i++){
    fill_array(A, 1000 * 1000);
    fill_array(B, 1000 * 1000);
    fill_array(C, 1000 * 1000);

    clock_gettime(CLOCK_MONOTONIC, &start);
// gemm_tt(M, N, K, ALPHA, A, lda, B, ldb, C, ldc);
    clock_gettime(CLOCK_MONOTONIC, &end_orig);

    compute(C_tt, ALPHA_tt, A_tt, B_tt, C_tt);
    clock_gettime(CLOCK_MONOTONIC, &end_taco);

    orig_run_times[i] = calc_spent_time(end_orig, start);
    taco_run_times[i] = calc_spent_time(end_taco, end_orig);
  }

  double orig_time = average(orig_run_times, n_runs);
  double taco_time = average(taco_run_times, n_runs);
  printf("%.5lf %.5lf", orig_time, taco_time);
  return 0;
}
